% COMP3419 Multimedia Assignment Report: Alive
% Daniel Buckmaster
% 

# Background

## Introduction

Recent debates have questioned the role of ICT in early childhood development,
especially in kindergartens.
Some educators believe bringing technology into play too early is detrimental
to the goals of an early education program:

> We don't want them sitting in front of a computer screen or a TV.
> They probably get enough of that at home.
> What they need at the centre is to run around, do something physical.
> Learn how to interact with other children.
> In early childhood that's what's important.
> The human touch.

On the other hand, some children still lack the ability to practise digital
literacy skills at home, even tasks as apparently-simple as learning to
manipulate a mouse or a keyboard.
University of Canberra researchers Anne Campbell and Grazia Scotellaro write:

> Although some of these skills are used for playing games,
> this is still an impressive array of digital literacy skills,
> even more so when they have been acquired more through independent learning
> and experimentation than through an adult providing instruction.

The debate may continue, but it seems inevitable that children will be increasingly
exposed to technology, whether at preschool, primary school, or at home.
It is important, then, to thoughtfully devise their interactions with this technology,
especially at the early childhood stage.
We must support the existing goals and methods of early childhood education
while providing new experiences.

## Application

I proposed and executed a visual 'toy' application that allows children to
interact with their artwork once it is scanned into the computer via a webcam.
The application detects distinct objects in a child's hand-painted image and
animates them in a virtual scene reconstructed from the scanned image.

The goal is to allow children to experience virtual interaction in a way that
integrates with their existing curriculum, and focuses not just on digital
creation, but digital enhancement of analog creation.

The application is called Alive, because that is what it does to still images.

## Requirements

 * The application shall run in a (modern) web browser
 * Automatic webcam detection
 * Object detection based on colour and silhouette
 * Non-realtime performance during image analysis
 * Realtime performance during scene rendering
 * Friendly user interface and experience

# System Design

## Processee drawing library

The system was developed and deployed with the Processee graphics library, a
wrapper over the Processing.js framework that encourages development in
CoffeeScript.
This involves the entire application living inside a web browser, taking
advantage of the WebRTC standard to get a webcam video stream.

The main application interface is a HTML page with the drawing canvas.
The Processee library is included as static JavaScript.
The actual application code is included as a CoffeeScript source file, which
is compiled to JavaScript by the browser on page load.

## User experience

Since the application does not require detailed user interaction, the user
interface is kept very simple.
The chart below outlines the flow of the user's interaction with the program.
Both 'button click' events are implemented as simple touch/click events on the
main program window.
This means there are no extraneous UI elements.

 ![](system.jpg)

The affordance of touching the main display was a question I considered during
development; in the end, I settled for changing the mouse cursor when it was over
the canvas.
Unfortunately, this does not improve affordance for touch interfaces, since
there is no mouse to observe changing.

In the end, I chose to add a small menu below the main display.
The menu allows the user to select two static images to try the program on, or
the webcam, and toggle debug rendering.
WHile these started as a convenience for me while during development, I decided
to leave them in as they provide a useful way to evaluate the system even without
a webcam connected, or a child's painting to hand.

## Algorithm design and implementation

### Process and innovation

The algorithm design process was a case of testing what would work with different
inputs and achieve the best results.
Some parts of the program, such as blob detection, are well-documented image
analysis tools.
Others, such as the hue-boundary object separation step or merging routine, are
my own invention.

The goal of this program is to transform a flat colour input image into a two-layer
output structure where each separate object detected in the input is free to move
independently of the rest of the image.
The meta-algorithm runs as follows:

 #. Separate the foreground of the input image from the background
 #. Separate the foreground into unique objects, recovering a rectangular
    bounding area around each
 #. For each foreground object create a 'sprite' whose appearance is the same
    as the input image
 #. Animate sprites when they are touched by the mouse

### Foreground detection

Initially, I wanted to perform automatic background detection.
However, I eventually decided to add the 'choose background' step to the system
flowchart to make foreground detection work better.
The process I devised for detecting the foreground has two parts.

The first is a simple colour match on the chosen background colour.
Pixels that are distant in colour-space from the background colour are considered
to be part of the foreground.
This results in a binary image, with white for the foreground and black for the
background, and is implemented in the `foreground` and `equalize` functions.

 ![Before and after the foreground separation step](foreground.jpg)

After obtaining a full-output-range greyscale image, it is dilated by one radial
pixel to provide a buffer around small or thin objects.
An example of the algorithm in action is above.

This step is implemented in CoffeeScript using a functional pipeline style:

```coffee
fgsep = @do dilate 1, @do equalize @do foreground col, source
```

This now differentiates foreground regions from background, but I wanted to go
further and distinguish using colour as well.
Many objects in simple images physically touch each other, but must be considered
separate, like a house sitting on grass.
Edge detection on the colour image is not effective enough for my purpose, unfortunately.

The solution I devised was to convert the colour input image to its HSV representation
and from there to a greyscale image containing only the H (hue) component.
This provided good distinction between regions of different colours that I could
perform edge detection on.
An example of the result is shown below:

 ![Before and after the hue-boundary step](hueboundary.jpg)

First the `toHue` funtion is used to transform the source image to its H representation
(the resulting image is saved as `col` for later use in object merging).
Then the result is passed through a `median` filter, as the hue process generates
noisy regions that are very effectively smoothed with this operator.
Finally, the Sobel `edge` operator detects the colour boundaries.
The entire step looks like this:

```coffee
colsep = @do edges @do median (col = @do toHue source)
```

When combined, these two steps give far greater object discrimination than either
of them alone. The small flowers in the first example image are a fantastic
case study:

 ![Left: original image. Centre: blobs with foreground detection only. Right: blobs with hue boundary.](huecomp.jpg)

### Object extraction and merging

Finally, the process of extracting 'blobs' from the image can begin.
Each 'blob' may represent an entire object, or just part of one (and may be
merged into a whole object at a later stage).
The two-pass blob extraction algorithm is a standard tool used to extract regions
of an image, so it is used here with little modification in the `blobs` function.
I should note that the algorithm works inside the image itself, using the RGB
channels to label regions.

An additional step in the algorithm uses the `col` image saved from the hue-boundary
pass above to calculate the medina hue of each blob.
This is used to help decide which objects should be merged into each other.

### Animation

To keep the project manageable (and because they're a reasonably uninteresting
part of the whole system), only fairly simple animations involving rotation and
scaling were implemented.
When a sprite is touched by the mouse, it starts an internal timer and calculates
the value of an animation function to determine its scale and rotation each frame.
These animations usually take the form of an exponentially decaying sine wave.
For example, one animation implemented is:

```coffee
@animation = (t) ->
  angle: 0.1 * (Math.exp -2 * t) * Math.sin 15 * t
  stop: t > 5
```

This function of time (`t`) returns an angle and a `stop` flag after 5 seconds.
The angle is a small decaying sine wave of the imput time.
Objects continue animating after the mouse has left them - until the `stop`
flag is returned by the animation function.

# System Evaluation

## Requirements fit

Overall, the system fits the requirements quite closely.
The two biggest problems in the final animation produced are small fragmentary
pieces that are not merged back into sensible objects, and objects that are
merged too greedily.
These are responsible for visual errors in the finished piece.

For example, this cloud with its thin outlines is broken to pieces by the
hue-boundary step, and is never fully recovered into a single object by merging.
Unfortunately I could determine no solution for thin objects like these.

 ![Thin outlines are handled poorly.](cloud.jpg)

At the other end of the spectrum, this sun has merged with two of the clearly-separate
ray objects, 

 ![An inappropriately greedy merge.](sun.jpg)

## Performance

Since realtime performance of the image processing algorithm was not required, I
was fairly carefree with its performance.
While I am not completely happy with the performance, I did not enter into any
optimisation, since I had more important tasks for the project.
There are some obvious performance-enhancing steps that could be taken both in
my application code and in the Processee framework:

 #. Reduce the number of new ImageData objects that are allocated by performing
    more operations in-place
 #. Optimise data structures such as the UnionFind class used in blob extraction
 #. Take advantage of web workers to parallelise large pixel loops

A more important performance improvement, however, would be to provide feedback
while the algorithm is running, instead of leaving the UI unresponsive.
Using web workers for more tasks, or even the entire image processing workload,
would be ideal and allow the main thread to continue updating the UI.

Realtime rendering performance of the reconstructed scene I have found to be acceptable.
Rendering sprites from image buffers is reasonably fast, and their animation
logic is not complicated.

## Framework and environment

I was personally disappointed with the workflow offered by Python's OpenCV
bindings, Processing, or Java.
This led to me creating Processee, a browser-based graphics environment which
originated with Bret Victor's critique of the Processing environment.
I believe this path gave me several advantages:

 * CoffeeScript is a modern, flexible language which was easy to develop in
 * I was developing in-browser, in the environment I would deploy in
 * The framework was developed with the needs of a complex project in mind,
   ensuring it was flexible and production-ready

Of course, there were downsides:

 * Developing an entire API took a lot of effort that could have been focused
   on developing algorithms and application code
 * Not using any existing framework left me without any example code to adapt or
   learn from

Fortunately, I think that neither of these disadvantages exceeded my abilities.
The Processee environment has become quite robust in the process, and I am very
happy with the application I built on top of it.

Using CoffeeScript was also a double-edged sword.
While I was especially happy with the functional style the language allowed me
to adopt (for example, the chained `@do` calls in the image processing steps),
the way CoffeeScript was included in the application (compiled client-side from
script tags) made it nearly impossible to debug runtime errors.

The web browser as a JavaScript interpreter is one of my favourite deployment
environments, but it is not without its limitations.
Most importantly, local access to the filesystem is very restricted, as are
other features (such as the webcam interface) when the site is opened as a file
instead of being served remotely.
ALso, limitations in the Canvas specification prevented me from, for example,
loading remote images via URL.
On the other hand, once the site is being served (either on the internet, or using
a local server such as WAMP or the Python script distributed with Alive),
the browser provides a convenient and ubiquitous platform with improving support
for peripherals such as the webcam.

# Conclusion


